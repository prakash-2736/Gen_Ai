# üìò LLM Inference ‚Äì Class Notes

## 1Ô∏è‚É£ What is LLM Inference?

**LLM Inference** is the process of using a **trained Large Language Model** to generate outputs (text, answers, summaries) from a given input prompt.

> Training = learning weights ‚ùå
> Inference = using learned weights to generate output ‚úÖ

---

## 2Ô∏è‚É£ Inference Pipeline (High Level)

```
Input Text ‚Üí Tokenizer ‚Üí Model ‚Üí Generated Tokens ‚Üí Decoder ‚Üí Output Text
```

### Components:

* **Tokenizer**: Converts text ‚Üí numbers (tokens)
* **Model**: Predicts next tokens
* **Decoder**: Converts tokens ‚Üí readable text

---

## 3Ô∏è‚É£ Popular Packages for LLM Inference

| Package        | Use Case                   |
| -------------- | -------------------------- |
| `transformers` | Most common (Hugging Face) |
| `torch`        | Model execution backend    |
| `accelerate`   | Multi‚ÄëGPU / optimization   |
| `vllm`         | High‚Äëspeed inference       |

---

## 4Ô∏è‚É£ Installing Required Packages

```bash
pip install transformers torch
```

---

## 5Ô∏è‚É£ Simple LLM Inference (Step‚Äëby‚ÄëStep)

### ‚úÖ Example Model

We use **GPT‚Äë2** because:

* Small size
* Works on CPU
* No authentication required

---

### üìå Code: Basic LLM Inference

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load tokenizer and model
model_name = "gpt2"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Input prompt
prompt = "Artificial Intelligence is"

# Tokenize input
inputs = tokenizer(prompt, return_tensors="pt")

# Generate output
outputs = model.generate(
    **inputs,
    max_new_tokens=50,
    temperature=0.7,
    do_sample=True
)

# Decode tokens to text
result = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(result)
```

---

## 6Ô∏è‚É£ Explanation of Important Parameters

| Parameter        | Meaning                                         |
| ---------------- | ----------------------------------------------- |
| `max_new_tokens` | Max tokens to generate                          |
| `temperature`    | Creativity control (0.2 = safe, 1.0 = creative) |
| `do_sample`      | Enables randomness                              |

---

## 7Ô∏è‚É£ Simplified Inference using Pipeline API

Best for **quick demos & teaching**.

```python
from transformers import pipeline

generator = pipeline("text-generation", model="gpt2")

output = generator(
    "Artificial Intelligence is",
    max_new_tokens=50
)

print(output[0]["generated_text"])
```

---

## 8Ô∏è‚É£ CPU vs GPU Inference

### CPU (Default)

```python
device = -1
```

### GPU (CUDA)

```python
device = 0
```

```python
generator = pipeline(
    "text-generation",
    model="gpt2",
    device=0
)
```

---

## 9Ô∏è‚É£ Real‚ÄëWorld Use Cases of LLM Inference

* Chatbots
* Resume parsing (ATS)
* Code generation
* Question answering
* RAG systems
* AI Agents

---

## üîü Key Takeaways

* Inference ‚â† Training
* Tokenizer + Model + Decoder = LLM
* `transformers` is the industry standard
* Pipeline API is best for beginners

---

## üß™ Assignment (Optional)

1. Change the prompt and observe output
2. Modify `temperature` values
3. Try another model (e.g., `distilgpt2`)

---

## üîú Next Class Topics

* Gemini API inference
* LLaMA local inference
* FastAPI inference server
* LLM Evaluation basics

---

‚úÖ *End of Notes*
